# Локальные модели

Локальные модели позволяют использовать Researcherry Coder без подключения к внешним API, обеспечивая приватность и независимость от интернета.

## Что такое локальные модели?

Локальные модели - это ИИ-модели, которые работают на вашем компьютере:

- **Приватность** - данные не покидают ваш компьютер
- **Независимость** - работа без интернета
- **Контроль** - полный контроль над моделью
- **Экономия** - нет платы за API вызовы

## Поддерживаемые локальные решения

### Ollama

**Популярное решение для локальных моделей**

#### Установка

```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# Скачайте с https://ollama.ai/download
```

#### Использование

```bash
# Скачивание модели
ollama pull llama3.2

# Запуск сервера
ollama serve

# Тестирование
ollama run llama3.2 "Привет, как дела?"
```

#### Настройка в Researcherry Coder

1. Откройте настройки Researcherry Coder
2. Перейдите в раздел "API Providers"
3. Выберите "Local Model"
4. Укажите URL: `http://localhost:11434`
5. Выберите модель из списка

### LM Studio

**Графический интерфейс для локальных моделей**

#### Установка

1. Скачайте с [lmstudio.ai](https://lmstudio.ai)
2. Установите приложение
3. Скачайте нужную модель

#### Настройка

1. Запустите LM Studio
2. Загрузите модель
3. Запустите локальный сервер
4. В Researcherry Coder укажите URL: `http://localhost:1234/v1`

### Text Generation WebUI

**Веб-интерфейс для локальных моделей**

#### Установка

```bash
git clone https://github.com/oobabooga/text-generation-webui
cd text-generation-webui
pip install -r requirements.txt
```

#### Запуск

```bash
python server.py --listen --api
```

#### Настройка в Researcherry Coder

URL: `http://localhost:5000/api/v1`

## Рекомендуемые модели

### Для программирования

- **CodeLlama** - специализированная для кода
- **WizardCoder** - отличная для программирования
- **Phind-CodeLlama** - оптимизированная для разработки

### Для общих задач

- **Llama 3.2** - сбалансированная модель
- **Mistral** - быстрая и эффективная
- **Gemma** - от Google, хорошая производительность

### Для русского языка

- **Saiga** - русскоязычная модель
- **GigaChat** - от Сбера (требует API ключ)

## Настройка производительности

### Системные требования

- **RAM:** минимум 8GB, рекомендуется 16GB+
- **GPU:** необязательно, но ускоряет работу
- **CPU:** современный процессор (Intel i5/AMD Ryzen 5+)

### Оптимизация памяти

```bash
# Для Ollama
ollama run llama3.2 --num-ctx 4096 --num-thread 4

# Для LM Studio
# Используйте настройки в интерфейсе
```

### GPU ускорение

```bash
# CUDA (NVIDIA)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Metal (Apple Silicon)
pip install torch torchvision torchaudio
```

## Сравнение с облачными моделями

### Преимущества локальных моделей

- ✅ Полная приватность
- ✅ Работа без интернета
- ✅ Нет лимитов использования
- ✅ Нет платы за API
- ✅ Полный контроль

### Недостатки локальных моделей

- ❌ Требуют мощное железо
- ❌ Меньше возможностей
- ❌ Медленнее облачных
- ❌ Сложнее настройка
- ❌ Ограниченный выбор моделей

## Гибридный подход

Вы можете комбинировать локальные и облачные модели:

### Настройка приоритетов

1. **Локальная модель** - для простых задач
2. **Облачная модель** - для сложных задач
3. **Автоматическое переключение** - при недоступности

### Пример конфигурации

```yaml
models:
    primary: "local-llama3.2"
    fallback: "openai-gpt-4"
    auto_switch: true
    local_threshold: 0.7
```

## Устранение неполадок

### Модель не загружается

1. Проверьте свободное место на диске
2. Убедитесь в стабильном интернете
3. Попробуйте другую модель

### Медленная работа

1. Увеличьте RAM
2. Используйте GPU
3. Уменьшите размер контекста
4. Оптимизируйте настройки

### Ошибки подключения

1. Проверьте, что сервер запущен
2. Убедитесь в правильности URL
3. Проверьте порты и файрвол

### Плохое качество ответов

1. Попробуйте другую модель
2. Настройте параметры (температура, токены)
3. Используйте более качественные промпты

## Советы по использованию

### 1. Выбирайте подходящую модель

- Для программирования → CodeLlama, WizardCoder
- Для общения → Llama, Mistral
- Для русского языка → Saiga

### 2. Оптимизируйте настройки

- Увеличьте RAM для больших моделей
- Используйте GPU для ускорения
- Настройте параметры под задачи

### 3. Комбинируйте модели

- Локальные для простых задач
- Облачные для сложных задач
- Автоматическое переключение

### 4. Регулярно обновляйте

- Следите за новыми моделями
- Обновляйте Ollama/LM Studio
- Тестируйте новые возможности

## Следующие шаги

Теперь, когда вы знакомы с локальными моделями, изучите:

- [Автоматическое одобрение действий](./auto-approving-actions.md)
- [MCP (Model Context Protocol)](./mcp.md)
